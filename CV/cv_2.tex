%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Medium Length Graduate Curriculum Vitae
% LaTeX Template
% Version 1.1 (9/12/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Rensselaer Polytechnic Institute (http://www.rpi.edu/dept/arc/training/latex/resumes/)
%
% Important note:
% This template requires the res.cls file to be in the same directory as the
% .tex file. The res.cls file provides the resume style used for structuring the
% document.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%    PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[margin, 10pt]{res} % Use the res.cls style, the font size can be changed to 11pt or 12pt here

\usepackage{helvet} % Default font is the helvetica postscript font
%\usepackage{newcent} % To change the default font to the new century schoolbook postscript font uncomment this line and comment the one above
\usepackage{hyperref}
\usepackage{pifont}
\usepackage[utf8]{inputenc}
\renewcommand{\baselinestretch}{1} 

% \usepackage[margin=0.6in]{geometry}
% \setlength{\textwidth}{6.1in} % Text width of the document
% \newsectionwidth{1.5in}

\begin{document}

%----------------------------------------------------------------------------------------
%    NAME AND ADDRESS SECTION
%----------------------------------------------------------------------------------------

\moveleft.5\hoffset\centerline{\large\bf \huge{Changan Chen}} % Your name at the top
 
\moveleft\hoffset\vbox{\hrule width \resumewidth height 1pt}\smallskip % Horizontal line after name; adjust line thickness by changing the '1pt'

% \moveleft.5\hoffset\centerline{45 Lansing St Apt 2612} % Your addre

% \moveleft.5\hoffset\centerline{San Francisco, San Francisco 94105}
% \moveleft.5\hoffset\centerline{(770) 401-6678}
% \moveleft.5\hoffset\centerline{changanvr@gmail.com}
% \moveleft.5\hoffset\centerline{github.com/changanvr}
% \moveleft.5\hoffset\centerline{changan.io}


%---------------------------------------------------------------------------------------



\begin{resume}

% \section{Contact}
% changanvr@gmail.com \\
% changan.io \\
% \href{https://scholar.google.com/citations?user=9Uxf0ikAAAAJ&hl=en}{Google Scholar} \\
% \href{https://github.com/ChanganVR}{GitHub} 

\section{Contact} 

Gates-Dell Complex, Room 4.728 \hfill +1 512 809 0167 \\
The University of Texas at Austin \hfill changan@cs.utexas.edu \\
Texas, 78712 \hfill \hyperlink{changan.io}{changan.io} \\
USA \hfill \href{https://github.com/changanvr}{GITHUB} \href{https://scholar.google.com/citations?user=9Uxf0ikAAAAJ&hl=en}{SCHOLAR}

\section{Education} 
\textbf{The University of Texas at Austin}, Austin, USA \hfill Sept. 2019 - Present \\
Ph.D. in Computing Science. \textbf{Expected graduation in May 2024.}

\textbf{Simon Fraser University}, Vancouver, Canada \hfill Sept. 2016 - Aug. 2019 \\
B.Sc in Computing Science Dual Degree Program with distinction. 
% GPA: 4.14/4.33

\textbf{Zhejiang University}, Hangzhou, China \hfill Sept. 2014 - Aug. 2016 \\
B.Eng in Computer Science Dual Degree Program with distinction. 
% GPA: 3.64/4.00

\section{Research \\ Interests}
Computer vision, sound and robotics \\ 
\textit{Audio-visual learning, embodied AI}

\section{Research \\ Experience}
\textbf{Graduate Research Assistant - Prof. Kristen Grauman}  \hfill Aug. 2019 - Present \\ 
Vision Lab, The University of Texas at Austin (UT Austin) \hfill Austin, USA

\textbf{Research Intern - Prof. Andrea Vedaldi}  \hfill July 2022 - Nov. 2022 \\ 
FAIR, Meta AI \hfill London, United Kingdom

% $\bullet$ Introduced the task of audio-visual navigation in complex visually realistic 3D environments.\\ 
% $\bullet$ Created a benchmark suite of tasks for audio-visual navigation to facilitate future work in this direction.

\textbf{Visiting Researcher}  \hfill May 2020 - June 2022 \\ 
Facebook AI Research (FAIR) \hfill Austin, USA
% $\bullet$ Improved previous audio-visual navigation models with a hierarchical model that learns to set waypoints\\ 
% $\bullet$ Introduced semantic audio-visual navigation task and proposed a transformer-based model to tackle it.

\textbf{Research Assistant - Prof. Manolis Savva}  \hfill Jan. 2019 - Aug. 2019 \\ 
Vision and Media Lab, Simon Fraser University (SFU) \hfill Vancouver, Canada 
% $\bullet$ Formulated the crowd navigation problem as graphs and use Graph Convolutional Network to encode higher-order interactions.\\ 
% $\bullet$ Used multi-step lookahead planning to further take into account the evolution of the human crowd.

\textbf{Research Intern - Prof. Alexandre Alahi}  \hfill May 2018 - Dec. 2018 \\ 
Visual Intelligence for Transportation Laboratory, \\
École Polytechnique Fédérale de Lausanne (EPFL) \hfill Lausanne, Switzerland
% $\bullet$ Enabled autonomous agents to navigate through crowds by modeling the crowd-robot interaction with 2D coordinate inputs. \\ 
% $\bullet$ Further formulated interaction modeling with visual input, which goes beyond agent-based planning to region-based planning.

\textbf{Research Assistant - Prof. Greg Mori} \hfill May 2017 - April 2018 \\ 
Vision and Media Lab, Simon Fraser University (SFU) \hfill Vancouver, Canada
% $\bullet$ Attempted to discover the set of action classes in sports videos with unsupervised learning by leveraging the video commentaries. \\
% $\bullet$ Studied overcoming the greediness of traditional linkage criteria in agglomerative clustering with reinforcement learning approach. \\
% $\bullet$ Investigated tacking the deep network compression under operational constraints from the perspective of constrained Bayesian optimization, which is guided by a novel constraint cooling strategy.

% \section{ArXiv Preprints}


\section{Publications}

\textbf{SoundingActions: Learning How Actions Sound from Narrated Egocentric Videos} \\
\textit{CVPR, 2024} \\
\textbf{Changan Chen}, Kumar Ashutosh, Rohit Girdhar, David Harwath, Kristen Grauman

\textbf{Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives} \\
\textit{CVPR, 2024} \\
Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, ..., \textbf{Changan Chen}, ..., Manolis Savva, Jianbo Shi, Mike Zheng Shou, Michael Wray

\textbf{Self-Supervised Visual Acoustic Matching} \\
\textit{NeurIPS, 2023} \\
Arjun Somayazulu, \textbf{Changan Chen}, Kristen Grauman

\textbf{Self-Supervised Visual Acoustic Matching} \\
\textit{NeurIPS, 2023} \\
Arjun Somayazulu, \textbf{Changan Chen}, Kristen Grauman

\textbf{Replay: Multi-modal Multi-view Acted Videos for Casual Holography} \\
\textit{ICCV, 2023} \\
Roman Shapovalov*, Yanir Kleiman*, Ignacio Rocco*, David Novotny, Andrea Vedaldi, \textbf{Changan Chen}, Filippos Kokkinos, Ben Graham, Natalia Neverova

\textbf{Novel-view Acoustic Synthesis} \\
\textit{CVPR, 2023} \\
\textbf{Changan Chen}, Alexander Richard, Roman Shapovalov, Vamsi Krishna Ithapu, Natalia Neverova, Kristen Grauman, Andrea Vedaldi

\textbf{Learning Audio-Visual Dereverberation} \\
\textit{ICASSP, 2023} \\
\textbf{Changan Chen}, Wei Sun, David Harwath, Kristen Grauman

\textbf{SoundSpaces 2.0: A Simulation Platform for Visual-Acoustic Learning} \\
\textit{Neural Information Processing Systems (NeurIPS), 2022} \\
\textbf{Changan Chen}*, Carl Schissler*, Sanchit Garg*, Philip Kobernik, Alexander Clegg, Paul Calamia, Dhruv Batra, Philip W Robinson, Kristen Grauman

\textbf{Few-Shot Audio-Visual Learning of Environment Acoustics} \\
\textit{Neural Information Processing Systems (NeurIPS), 2022} \\
Sagnik Majumder, \textbf{Changan Chen}, Ziad Al-Halah, Kristen Grauman

\textbf{Visual Acoustic Matching} \\
\textit{Conference on Computer Vision and Pattern Recognition (CVPR), 2022 (\textbf{Oral})} \\
\textbf{Changan Chen}, Ruohan Gao, Paul Calamia, Kristen Grauman

\textbf{Sound Adversarial Audio-Visual Navigation} \\
\textit{International Conference on Learning Representations (ICLR), 2022} \\
Yinfeng Yu, Wenbing Huang, Fuchun Sun, \textbf{Changan Chen}, Yikai Wang, Xiaohong Liu

\textbf{Semantic Audio-Visual Navigation} \\
\textit{Conference on Computer Vision and Pattern Recognition (CVPR), 2021} \\
\textbf{Changan Chen}, Ziad Al-Halah, Kristen Grauman

\textbf{Learning to Set Waypoints for Audio-Visual Navigation} \\
\textit{International Conference on Learning Representations (ICLR), 2021} \\
\textbf{Changan Chen}, Sagnik Majumder, Ziad Al-Halah, Ruohan Gao, Santhosh K. Ramakrishnan, Kristen Grauman

\textbf{SoundSpaces: Audio-Visual Navigation in 3D Environments} \\
\textit{European Conference on Computer Vision (ECCV) 2020 (\textbf{Spotlight, top 5\%})} \\
\textbf{Changan Chen*}, Unnat Jain*, Carl Schissler, Sebastia Vicenc Amengual Gari, Ziad Al-Halah, Vamsi Krishna Ithapu, Philip Robinson, Kristen Grauman

\textbf{VisualEchoes: Spatial Image Representation Learning through Echolocation} \\
\textit{European Conference on Computer Vision (ECCV) 2020} \\
Ruohan Gao, \textbf{Changan Chen}, Carl Schissler, Ziad Al-Halah, Kristen Grauman

\textbf{Relational Graph Learning for Crowd Navigation} \\
\textit{International Conference on Intelligent Robots and Systems (IROS) 2020} \\
\textbf{Changan Chen*}, Sha Hu*, Greg Mori, Manolis Savva

\textbf{Crowd-Robot Interaction: Crowd-aware Robot Navigation with Attention-based Deep Reinforcement Learning} \\
\textit{International Conference on Robotics and Automation (ICRA) 2019} \\
\textbf{Changan Chen}, Yuejiang Liu, Sven Kreiss, Alexandre Alahi

\textbf{Constraint-aware Deep Neural Network Compression} \\
\textit{European Conference on Computer Vision (ECCV) 2018} \\
\textbf{Changan Chen}, Fred Tung, Naveen Vedula, Greg Mori


\section{Talks}
4D Audio-Visual Perception: Simulating, Synthesizing and Navigating with Sounds in Spaces \hfill Dec. 2023 \\
\textit{\footnotesize{Invited talk, MARL lab, NYU}} \\
Audio-Visual Embodied AI: From Simulating to Navigating with Sounds in Spaces \hfill June 2023 \\
\textit{\footnotesize{Keynote talk, PerDream Workshop, ICCV 2023}} \\
Novel-view Acoustic Synthesis \hfill June 2023 \\
\textit{\footnotesize{Keynote talk, Sight and Sound Workshop, CVPR 2023}} \\
Visual-acoustic Learning \hfill June 2023 \\
\textit{\footnotesize{Keynote talk, Ambient AI Workshop, ICASSP 2023}} \\
Visual Learning of Sound in Spaces \hfill Feb 2023 \\
\textit{\footnotesize{Invited talk, Texas Acoustics}} \\
Visual Learning of Sound in Spaces \hfill Jan 2023 \\
\textit{\footnotesize{Invited talk, MIT CDFG group}} \\
Visual Learning of Sound in Spaces \hfill Nov 2022 \\
\textit{\footnotesize{Invited talk, FAIR, Meta AI}} \\
Visual Acoustic Matching \hfill June 2022 \\
\textit{\footnotesize{Oral talk, CVPR 2022}} \\
Semantic Audio-Visual Navigation \hfill June 2021 \\
\textit{\footnotesize{Invited talk at \href{https://eyewear-computing.org/EPIC_CVPR21/program}{The Eighth EPIC Workshop, CVPR 2021}}}\\
Audio-Visual Navigation \hfill Sept. 2020 \\
\textit{\footnotesize{Invited talk at \href{https://www.cs.utexas.edu/~yukez/cs391r_fall2020/}{Robot Learning course at UT Austin}}} \\
Constraint-aware Deep Neural Network Compression  \hfill Sept. 2018 \\
\textit{\footnotesize{Poster session presented at ECCV 2018, Munich, Germany}} \\
Constraint-aware Deep Neural Network Compression \hfill May 2018 \\
\textit{\footnotesize{Poster presentation at SFU-ZJU Joint Symposium, Hangzhou, China}} \\
Navigation in Crowds: From 2D Navigation to Visual Navigation \hfill Dec. 2018 \\
\textit{\footnotesize{Invited Talk at SwissAI Meetup, Lausanne, Switzerland}} \\
Crowd-aware Robot Navigation with Attention-based Deep Reinforcement Learning \\
\textit{\footnotesize{Invited Talk at Swiss Machine Learning Day, Lausanne, Switzerland}} \hfill Nov. 2018 


\section{Professional \\ Service}
Organizing \href{https://multimodalitiesfor3dscenes.github.io/}{Multimodalities for 3D Scenes}, CVPR 2024 \\
Organizing \href{https://av4d.org}{AV4D Workshop}, ECCV 2022, ICCV 2023 \\
Co-organizing \href{https://www.l3das.com/icassp2023/}{L3DAS23: Learning 3D Audio Sources for Audio-Visual Extended Reality}, ICASSP 2023 \\
Co-organizing \href{https://embodied-ai.org/}{Embodied AI Workshop}, CVPR 2023\\
Organizing \href{https://soundspaces.org/challenge}{SoundSpaces Challenge} at the Embodied AI Workshop, CVPR'21, 22, 23.

Serving as reviewers for CVPR, ECCV, ICCV, NeurIPS, ICLR, IROS, ICRA, RA-L, and SIGGRAPH.


\section{Honors \& \\ Awards}
Spring Dissertation Fellowship \hfill Dec 2024\\
Professional Development Awards 2022 \hfill July 2022 \\
Adobe Research Fellowship 2022 \hfill Jan. 2022 \\
Awarded the BS degree with Distinction \hfill Aug. 2019 \\ 
President's \& Dean's Honour Roll \hfill Oct. 2017 \\
SFU Alumni Scholarship \hfill Sept. 2017 \\
% SFU Alumni Scholarship \hfill Jan. 2017 \\
Simon Fraser University Entrance Award \hfill Sept. 2016

\section{Media \\ Coverage}
\textbf{Axios},
\href{https://www.axios.com/2022/06/24/meta-wants-the-metaverse-to-sound-more-like-the-real-world?utm_source=twitter&utm_medium=social&utm_campaign=editorial&utm_content=technology-metaverse}{Meta wants the metaverse to sound more like the real world} \hfill June 2021\\
\textbf{Digital Information World},
\href{https://www.digitalinformationworld.com/2022/06/meta-plans-on-taking-digital.html}{Meta Plans On Taking Digital Experiences To The Next Level With The Creation Of Audio Tools For AR And VR} \hfill June 2021\\
\textbf{Engadget},
\href{https://www.engadget.com/metas-latest-auditory-ai-promise-a-more-immersive-ar-vr-experience-130029625.html?guccounter=1&guce_referrer=aHR0cHM6Ly9sLndvcmtwbGFjZS5jb20v&guce_referrer_sig=AQAAAAwu3kFYVTr7weTarFhe5AHNg9vLok3QfxATQBYtXFYSsZmQkgXIB2s8PHHuclDkYp8w7DA0_oFtTEerWPcxx21S4zDM3ow_vYDF-aClHJaSnf_wJ8nkfGvlsuNKvqo1Y9r1L3Dv0xwUXaTr8BObcNwzx7h9bjLVV6LdZbw2sxr-}{Meta's latest auditory AIs promise a more immersive AR/VR experience} \hfill June 2021\\
\textbf{TechMonitor}, \href{https://techmonitor.ai/technology/emerging-technology/meta-audio-ai-metaverse}{Sound of the metaverse: Meta creates AI models to improve virtual audio} \hfill June 2021\\
\textbf{TechRadar}, \href{https://www.techradar.com/news/meta-wants-the-virtual-landscape-to-sound-like-real-life}{Meta wants the virtual landscape to sound like real life} \hfill June 2021\\
\textbf{SiliconANGLE}, \href{https://siliconangle.com/2022/06/24/meta-building-better-ai-driven-audio-virtual-reality/}{Meta is building better AI-driven audio for virtual reality} \hfill June 2021\\
\textbf{Social Media Today}, \href{https://www.socialmediatoday.com/news/metas-developing-new-spatial-audio-tools-for-ar-and-vr-to-enhance-virtual/626071/?utm_source=dlvr.it&utm_medium=twitter}{Meta’s Developing New Spatial Audio Tools for AR and VR to Enhance Virtual Experiences} \hfill June 2021\\
\textbf{Facebook AI Blog}, \href{https://ai.facebook.com/blog/2021-habitat-challenge-launches-to-advance-embodied-ai-research/}{2021 Habitat Challenge launches to advance embodied AI research} \hfill Feb. 2021\\
\textbf{Facebook AI Blog}, \href{https://ai.facebook.com/blog/new-milestones-in-embodied-ai}{New milestones in embodied AI} \hfill Aug. 2020\\
\textbf{VentureBeat}, \href{https://venturebeat.com/2020/08/21/facebook-releases-tools-to-help-ai-navigate-complex-environments}{Facebook releases tools to help AI navigate complex environments} \\
\textbf{ZDNet}, \href{https://www.zdnet.com/article/facebook-is-building-robots-to-help-you-find-your-ringing-phone}{Facebook is building home robots to help you find your ringing phone} \\
\textbf{MIT Technology Review}, \href{https://www.technologyreview.com/2020/08/21/1007523/facebook-ai-robot-assistants-hear-and-see}{Facebook is training robot assistants to hear as well as see}


\end{resume}
\end{document}